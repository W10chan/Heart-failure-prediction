{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3Tsa97EF8C9T8CHg1D7ZZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"sBu3k-O69CsC","executionInfo":{"status":"error","timestamp":1711174858161,"user_tz":-540,"elapsed":252,"user":{"displayName":"Masatomo Watanabe","userId":"16418028500894082517"}},"outputId":"ae57ab8e-82e6-4edd-edfc-3adf84942b91"},"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"expected an indented block after function definition on line 38 (<ipython-input-5-8abacbfb589f>, line 42)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-8abacbfb589f>\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    class Environment:\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 38\n"]}],"source":["# Bergman最小モデルに食事摂取量と身体活動を組み込んだモデル\n","def bergman_model_with_meal_and_activity(C, t, k, meal, activity):\n","    # 食事による血糖値の上昇\n","    meal_effect = meal * np.exp(-0.1 * t)\n","    # 身体活動による血糖値の減少\n","    activity_effect = activity * t\n","    # Bergman最小モデルの微分方程式\n","    dCdt = -k * C + meal_effect - activity_effect\n","    return dCdt\n","\n","# シミュレーションの実行\n","def simulate_blood_glucose(k, meal, activity):\n","    t = np.linspace(0, 10, 100)  # 時間軸\n","    C0 = initial_blood_glucose  # 初期血糖値\n","    C = odeint(bergman_model_with_meal_and_activity, C0, t, args=(k, meal, activity))\n","    return t, C\n","\n","# 強化学習エージェントのクラス\n","class Agent:\n","    def __init__(self, alpha, gamma, epsilon):\n","        self.alpha = alpha  # 学習率\n","        self.gamma = gamma  # 割引率\n","        self.epsilon = epsilon  # 探索率\n","        self.Q = {}  # 状態行動価値関数\n","\n","    def choose_action(self, state):\n","        if np.random.uniform(0, 1) < self.epsilon:\n","            # ε-greedy方策による探索\n","            return np.random.choice([0, 1])  # インスリン注入量を0または1でランダムに選択\n","        else:\n","            # 最適行動を選択\n","            if state in self.Q:\n","                return np.argmax(self.Q[state])\n","            else:\n","                # 状態が未知の場合はランダムに行動を選択\n","                return np.random.choice([0, 1])\n","\n","    def learn(self, state, action, reward, next_state):\n","        # ... (rest of the code)\n","\n","# 環境の定義\n","class Environment:\n","    def __init__(self, k, meal, activity):\n","      self.k = k\n","      self.meal = meal\n","      self.activity = activity\n","# パラメータ\n","k = 0.1  # 除去速度定数 (1/hour)\n","meal = 100  # 食事摂取量\n","activity = 5  # 身体活動量\n","target_blood_glucose = 100  # 目標血糖値\n","\n","# エージェントの初期化\n","alpha = 0.1  # 学習率\n","gamma = 0.9  # 割引率\n","epsilon = 0.1  # 探索率\n","agent = Agent(alpha, gamma, epsilon)\n","\n","# 環境の初期化\n","env = Environment(k, meal, activity)\n","\n","# エピソード数\n","num_episodes = 1000\n","\n","# エージェントの学習\n","for episode in range(num_episodes):\n","    state = (initial_blood_glucose, meal, activity)\n","    #state = ','.join(map(str, [initial_blood_glucose, meal, activity]))\n","\n","    while True:\n","        # 行動の選択\n","        action = agent.choose_action(state)\n","        # 行動の実行\n","        next_blood_glucose, reward = env.step(action)\n","        # 次の状態\n","        next_state = (next_blood_glucose, meal, activity)\n","        #next_state = ','.join(map(str, [next_blood_glucose, meal, activity]))\n","\n","        # 学習\n","        agent.learn(state, action, reward, next_state)\n","        state = next_state\n","        # 終了条件\n","        if abs(next_blood_glucose - target_blood_glucose) < 1:\n","            break\n","\n","# 学習後のQ値を表示\n","print(\"Learned Q-values:\")\n","print(agent.Q)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"-Xw6sOye-DKf"},"execution_count":null,"outputs":[]}]}